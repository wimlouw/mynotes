---
title: 'Notes on webscraping'
author: Wim Louw
date: '2020-11-14'
slug: webscraping
categories: []
tags: []
---

```{r, include=FALSE}

knitr::opts_chunk$set(
  comment = ">", message = FALSE, warning = FALSE
)

```

## Intro

A couple of years ago, I had the opportunity to work as a Research Associate on a labour market study. Part of this study involved looking for suitable job vacancies and applying for jobs on behalf of participants. It involved aggregating job postings across a few sites, selecting an appropriate subset of jobs (by location, experience, and other employer requirements), and careful curation. The process was greatly helped by webscraping scripts we developed.[^1] I was able to learn a bit about how to programmatically extract information from websites using [R](https://www.r-project.org/about.html), [rvest](https://github.com/tidyverse/rvest), and [R Selenium](https://docs.ropensci.org/RSelenium/).[^2]

[^1]: In practice, this aggregation process involved both webscraping, and a fair amount of manual searching!

[^2]: I have some R and RStudio set-up and other resources [here](https://wimlouw.netlify.app/2020/09/24/resources/#r)

What is webscraping about? Well it's mostly about getting the things you're seeing on a webpage into a form you can work with on your computer: a dataset, a list, or whatever. This might be a single table or multiple tables across many pages; or it might be a combination of elements, like job ads with associated information.

In essence, this is a data extraction and cleaning exercise! And in the examples below, it's about getting the content you want from the HTML (i.e. parsing HTML).

Once you have the content in a form you can work with, it's standard data cleaning stuff (manipulating strings, dates, &c.[^3]).

[^3]: Check out [R for Data Science's](https://r4ds.had.co.nz/index.html) chapters on dates and strings; then: (1) [Simple, Consistent Wrappers for Common String Operations â€¢ stringr,](https://stringr.tidyverse.org/) (2) [GitHub - trinker/textclean: Tools for cleaning and normalizing text data,](https://github.com/trinker/textclean) (3) [Quantitative Analysis of Textual Data â€¢ quanteda](https://quanteda.io/) (text analysis)

A lot of the work involves figuring out how the website works: where the information you want is, choosing the right [css-selectors](https://www.w3schools.com/cssref/css_selectors.asp), working with URLs[^4] , and figuring out how to iterate (or "crawl") between pages.

[^4]: See Julia Evans' "[how URLs work](https://wizardzines.com/comics/how-urls-work/)"; also see Oliver Keyes & Jay Jacobs' "[urltools](https://github.com/Ironholds/urltools)" R package --- not something I'm getting into, but I think working with URLs programmatically is the way to go!

In more complicated cases, it involves writing a script that can interact with the webpage (by driving a browser), to reveal the content you want. If you want to build something more sophisticated, it might involve working with a database, a scheduler, and other tools.[^5] You might want to build something that extracts information regularly, tracks changes over time and stores it in a database.[^6]

[^5]: Have a look at [Heroku](https://www.heroku.com/) and their "Hobby-dev" PostgreSQL with the fee Heroku scheduler

[^6]: [This](https://simonwillison.net/2020/Oct/9/git-scraping/) is interesting: Simon Willison's "Git scraping: track changes over time by scraping to a Git repository"

The examples below are all quite "basic": (1) Getting a table on Wikipedia (trickier than I thought!), and (2) getting information about jobs on a site called Gumtree (.co.za) .

## Considerations

There are a few things to consider before investing time in writing a script to extract information from a webpage (or pages):

-   **Is this already available?** Perhaps it can be downloaded from the site as a spreadsheet already (have a look around), or perhaps the site has an API[^7] you can access to download data.
-   **Is there a low effort way of doing this?** For instance, maybe you can just copy-paste what you need into a spreadsheet or directly into R[^8]? Or maybe there's a product or function you can use straight-away.[^9]
-   **How much copy-paste/ "low-effort" work are we talking here?** I think writing a simple script can, in many cases, be the lowest effort (and cheapest) of all, but one might hesitate because it feels too daunting. If you'll need to copy-paste multiple things across multiple pages, rather write a script! And if it's something you plan to do regularly --- automate with a script.
-   **Is it OK for me to do this?** This is a trickier question. It depends. And sometimes it's hard to tell! Things you can do: check if the site has a `robots.txt` file[^10] (which, if a site has one, includes do's and don't for scrapers/crawlers); ask yourself "would I feel OK about this information being pulled if this was my website?". If you decide to scrape --- be "polite" about it: don't overwhelm the site. Don't bother the site more than you need to, i.e. don't send too many requests to a website per second. You can avoid this by trying to be efficient (don't repeatedly query the website); include some "rests" in your code, to slow things down, to emulate regular browsing.

[^7]: See [What exactly IS an API? - by Perry Eising on Medium](https://medium.com/@perrysetgo/what-exactly-is-an-api-69f36968a41f)

[^8]: See Miles McBain's "[datapasta](https://github.com/MilesMcBain/datapasta)" package

[^9]: See, for instance, [Octoparse](https://www.octoparse.com/) or [webscraper.io](https://webscraper.io/). Also see, Google Sheets' `importHTML` function (see the documentation [here](https://support.google.com/docs/answer/3093339?hl=en) or see this Martin Hawksey's tutorial [here](https://mashe.hawksey.info/2012/10/feeding-google-spreadsheets-exercises-in-import/) (looks good)

[^10]: See <https://en.wikipedia.org/wiki/Robots_exclusion_standard> then, there's helpful information in Peter Meissner's `robotstxt` R package [documentation](https://cran.r-project.org/web/packages/robotstxt/vignettes/using_robotstxt.html)

## The front-end of the web[^11]

[^11]: See <https://www.internetingishard.com/html-and-css/introduction/> and <https://www.w3schools.com/html/html_intro.asp>

What you see when you open a webpage is brought to you by:

-   **HTML** -- basic content and layout semantics (adding structure to raw content)
-   **CSS** -- adding styles (formatting, colours, sizes of things, sometimes where things are placed, &c.)
-   **JavaScript** - making things interactive (behaviour of things on the page)

[Here](https://codepen.io/rcyou/pen/QEObEk/) is an example on [CodePen](https://codepen.io/) bringing these together.

You can also see how these come together by using your browser's inspect tool. On Chrome you can right click and select "inspect" or go to More Tools/ Developer Tools[^12].

[^12]: <https://developers.google.com/web/tools/chrome-devtools/open>

Here's what it looks like:

[![A screenshot of the J-PAL Africa Research webpage viewed with the Chrome inspection tool](/img/Screenshot 2020-11-12 at 10.38.08.png "A screenshot of the J-PAL Africa Research webpage viewed with the Chrome inspection tool"){#inspection-example}](https://www.povertyactionlab.org/page/research-south-africa)

## Tools

I used a combination of the inspect tool and [SelectorGadget](http://selectorgadget.com/) ("point and click CSS selectors") to identify what the stuff (the "elements") I wanted to extract are called. I will use the [rvest](https://github.com/tidyverse/rvest) R package ("rvest helps you scrape information from webpages").[^13] And a number of [tidyverse tools](https://www.tidyverse.org/) (for data manipulation).

[^13]: Pretty complete examples on how to use rvest here: <http://rvest.tidyverse.org/>

## Examples

### Set up, due diligence

I'll be using the following packages:

```{r}

library(tidyverse) ## for data manipulation (various packages)
library(rvest) ## for scraping
library(textclean) ## some cleaning functions
library(robotstxt) ## to get robotstxt protocols
library(kableExtra) ## for printing out an html table in this post

```

I'm going to be scraping from Wikipedia.org and Gumtree.co.za. I want to check if it's OK to do that ðŸ¤”

Wikipedia has a robots.txt, and it seems OK to scrape if you're going to be polite about it:

```{r}

rtxt_wikipedia <- robotstxt(domain="wikipedia.org")
replace_html(str_trunc(rtxt_wikipedia$text, 500, "right"))

```

As for Gumtree.co.za --- it has one, but I'm finding it hard to interpret the exclusions criteria...I think I'll just be very polite Â¯\\\_(ãƒ„)\_/Â¯

### Tables

In the first example, I want to extract a table from Wikipedia:

[![A screenshot of the 2019 South African general election results on Wikipedia](/img/Screenshot 2020-11-14 at 20.34.27.png "A screenshot of the 2019 South African general election results on Wikipedia")](https://en.wikipedia.org/wiki/Elections_in_South_Africa)

The first step is to use rvest's `read_html` function:

```{r}

## get the website HTML
website <- read_html("https://en.wikipedia.org/wiki/Elections_in_South_Africa") 

```

Now I want to identify the table I'm seeing so I can parse it from the HTML using rvests's `html_table` function.

ðŸ‘€...ðŸ‘€...ðŸ‘€

I tried using SelectorGadget...and the Chrome inspection tool. Hmmm, OK, this is actually tricky on Wikipedia, I can't find the specific css-selector on this page.

**But** something you an do (after some Googling) is to get **all tables** on the page; then you can identify which one is the right one using string matching (the `grep` function, below):

```{r}

table <- html_nodes(website, "table") ## all tables
length(table) ## how many tables?

## I want the one that contains "ANC" and has ~55 rows
data <- html_table(table[grep("ANC", table)], fill = T)[[1]]

## let's see
glimpse(data)
head(data) ## great!

```

### Other elements

Let's try something else. Getting job information on Gumtree.co.za

[![Screenshot of job postings on gumtree.co.za (example)](/img/Screenshot 2020-11-14 at 20.47.23.png "Screenshot of job postings on gumtree.co.za")](https://www.gumtree.co.za/s-jobs/v1c8p1)

**Things to note here:** descriptions are truncated, and contact information is hidden. To reveal them, you need to click on these elements. For the purposes of these examples, we're not going to get into headless browsers, and how to get your script to interact with page elements (but this is possible!).

Let's try getting information about a single job. I looked for "entry level" jobs, and will read the HTML on the first page. Using the inspect tool, I see the the job title elements are named `.related-ad-title` :

```{r}

jobs <- read_html("https://www.gumtree.co.za/s-jobs/page-1/v1c8p2?q=entry+level")

links <-
  jobs %>%
  html_nodes(".related-ad-title") %>%
  html_attr("href")

head(links)

```

Great now I have all the links on page 1!

I want to look at the first one, and extract some information. For the example I'm extracting the job title, description, job type, employer, and also including a time-stamp. I use the inspect tool I identify each element's css-selector. There's much more information on these pages than the few I chose for this example. The approach is the same!

```{r}

## first link
jobs_1 <- read_html(paste0("https://www.gumtree.co.za", links[1]))

title <- jobs_1 %>%
  html_nodes("h1") %>%
  html_text() %>%
  str_squish()

title

```

This worked. I will need to do this for a bunch of things --- title, description, &c. So I'm going to make a small function to make that easier, and to avoid repeating myself too much:

```{r}

## function for cleaning single elements
cleanup <- function(site, tag) {
  output <- site %>%
    html_nodes(tag) %>%
    html_text() %>%
    str_squish() ## remove leading and trailing whitespace
  output <- ifelse(length(output) == 0, NA, output) ## in case an element is missing
  return(output)
}

## let's try it out!
description <- cleanup(jobs_1, ".description-website-container") ## got these with SelectorGadget
jobtype <- cleanup(jobs_1, ".attribute:nth-child(4) .value")
employer <- cleanup(jobs_1, ".attribute:nth-child(3) .value")
time <- Sys.time()

## and put it into a tibble (see "https://r4ds.had.co.nz/tibbles.html")
dat <-
  tibble(
    title,
    description,
    jobtype,
    time
  )

dat ## looks good!

```

What if I wanted to do this for all ads on the page?

### Iteration

I have the links from the first page. I want to extract all the information I specified above for each one.

I think I'll write another function, which will create a tibble[^14] for every link, and then I'll put this into a list of tibbles using a for loop:

[^14]: <https://r4ds.had.co.nz/tibbles.html>

```{r}

## function for cleaning all elements I want on a page
cleanup2 <- function(x) {
  page <- read_html(x)
  title <- cleanup(page, "h1") ## "cleanup" for cleaning individual elements (from before)
  description <- cleanup(page, ".description-website-container")
  jobtype <- cleanup(page, ".attribute:nth-child(4) .value")
  employer <- cleanup(page, ".attribute:nth-child(3) .value")
  time <- Sys.time()
  dat <-
    tibble(
      title,
      description,
      jobtype,
      time
    )
  return(dat)
}

## fixing the link
links <- paste0("https://www.gumtree.co.za", links)
links <- sample(links, 7) ## sampling 7 random links for the example

## a list (for storing tibbles), and a for loop to iterate through links
output <- list()
for (i in 1:length(links)) {
  deets <- cleanup2(links[i])
  deets$title
  output[[i]] <- deets # add to list
  Sys.sleep(1) ## resting
}

## combining all tibbles in the list into a single big tibble
all <- bind_rows(output) ## Fab!


```

OK, let's see what this looks like (I'm truncating job descriptions for display):

```{r}

## truncate
all$description <- str_trunc(all$description, 100, "right")

## I'm using kable to make an html table for the site
all %>%
  kable() %>%
  kable_paper(bootstrap_options = "striped", full_width = F)

```

Nice! I just did the first page.

How would one do other pages?

One (hacky) approach is to use the URL and another for loop.[^15] In the example below, I'll extract the job links for the first 3 pages:

[^15]: There are better ways of working with URLs: <https://github.com/Ironholds/urltools>

```{r}

pages <- 3 ## let's just do 3
job_list <- list()
link <- "https://www.gumtree.co.za/s-jobs/page-" ## url fragment

for (i in 1:pages) {
  jobs <-
    read_html(paste0(link, i, "/v1c8p2?q=entry+level")) ## using paste
  links <-
    jobs %>%
    html_nodes(".related-ad-title") %>%
    html_attr("href") # get links
  job_list[[i]] <- links # add to list
  Sys.sleep(1)
}

links <- unlist(job_list)
head(links, 10) ## looks good!

```

I could then use the same approach to cycle through this much longer list of links.

### Things to think about

Make sure your script can handle errors.[^16]

[^16]: See "safely" and "possibly" from the [purrr](https://purrr.tidyverse.org/) package (or "trycatch")

-   css-selectors change
-   links stop working (page names change, or they stop existing)
-   not all pages are necessarily the same (some elements may be missing from some site) --- see the "job type" variable above, for instance
-   sites time out (that can break your script if you don't account for it)
-   &c.

------------------------------------------------------------------------

## Resources

-   Mine Dogucu & Mine Ã‡etinkaya-Rundel (2020): "[Web Scraping in the Statistics and Data Science Curriculum: Challenges and Opportunities](https://github.com/mdogucu/web-scrape)", Journal of Statistics Education
-   [How we learnt to stop worrying and love web scraping \| Nature Career Column](https://www.nature.com/articles/d41586-020-02558-0) by Nicholas J. DeVito, Georgia C. Richards & Peter Inglesby
-   [Chapter 12: "Web scraping"](https://automatetheboringstuff.com/2e/chapter12/) in "Automate the Boring Stuff with Python" by Al Sweigart
-   [Git scraping: track changes over time by scraping to a Git repository](https://simonwillison.net/2020/Oct/9/git-scraping/)
-   Headless browing: [R Bindings for Selenium WebDriver â€¢ rOpenSci: RSelenium](https://docs.ropensci.org/RSelenium/)
-   Ethics/best practice
    -   [Robots exclusion standard - Wikipedia](https://en.wikipedia.org/wiki/Robots_exclusion_standard)
    -   [Introduction to robots.txt - Search Console Help](https://support.google.com/webmasters/answer/6062608?hl=en)
    -   [GitHub - dmi3kno/polite: Be nice on the web](https://github.com/dmi3kno/polite)
-   More implementation
    -   [A Crazy Little Thing Called {purrr} - Part 1 : Web Mining - Colin Fay](https://colinfay.me/purrr-web-mining/)
-   String manipulation
    -   [RegExr: Learn, Build, & Test RegEx](https://regexr.com/)
    -   [Regex Tester and Debugger Online - Javascript, PCRE, PHP](https://www.regextester.com/)
    -   R cheatsheets: [RStudio Cheatsheets - RStudio](https://rstudio.com/resources/cheatsheets/)
    -   [GitHub - kevinushey/rex: Friendly regular expressions for R.](https://github.com/kevinushey/rex)
    -   [rebus: README.md](https://rdrr.io/cran/rebus/f/README.md)
    -   [GitHub - VerbalExpressions/RVerbalExpressions: Create regular expressions easily](https://github.com/VerbalExpressions/RVerbalExpressions)
-   See also
    -   [Data science for economists \| Grant McDermott](https://www.google.com/url?q=https%3A%2F%2Fgithub.com%2Fuo-ec607%2Flectures&sa=D&sntz=1&usg=AFQjCNHFFTznBoeUT4oCXc_ILvRmBa1gnQ)
    -   [Beautiful Soup Documentation --- Beautiful Soup 4.9.0 documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
    -   [RoboBrowser: Your friendly neighborhood web scraper --- robobrowser 0.1 documentation](https://robobrowser.readthedocs.io/en/latest/readme.html)
    -   [GitHub - r-lib/httr: httr: a friendly http package for R](https://github.com/r-lib/httr)
