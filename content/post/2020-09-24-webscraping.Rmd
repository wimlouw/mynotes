---
title: 'Notes on webscraping'
author: Wim Louw
date: '2020-11-14'
slug: webscraping
categories:
  - code
  - tutorials
tags:
  - webscraping
  - html
  - CSS-selectors
  - rvest
output:
  blogdown::html_page:
    toc: yes
---

```{r set-up, include=FALSE}
## Global chunk options ####
knitr::opts_chunk$set(
  comment = ">", message = FALSE, warning = FALSE
)

```

## Intro

A couple of years ago, I had the opportunity to work as a Research Associate on a labour market study in South Africa. Part of this study involved looking for suitable online job vacancies and applying for jobs on behalf of participants.

The process was greatly helped by webscraping scripts we developed.[^1] And I learned a bit about how to programmatically extract information from websites using [R](https://www.r-project.org/about.html), [rvest](https://github.com/tidyverse/rvest), and [R Selenium](https://docs.ropensci.org/RSelenium/).[^2]

[^1]: The process involved both webscraping, and a fair amount of manual searching!

[^2]: R and RStudio set-up and other resources [here](https://wimlouw.netlify.app/2020/09/24/resources/#r)

Webscraping is about getting the thing you're seeing on a webpage into a form you can work with on your computer: a dataset, a list, or whatever. It might be single table or multiple tables across many pages; or a combination of elements, like job ads with associated information.

This is a data extraction and cleaning exercise: getting the content you want from the page HTML. Once you have the content in a form you can work with, it's standard data cleaning stuff (manipulating strings, dates, &c.[^3]).

[^3]: See [R for Data Science's](https://r4ds.had.co.nz/index.html) chapters on dates and strings; then: (1) [Simple, Consistent Wrappers for Common String Operations • stringr,](https://stringr.tidyverse.org/) (2) [GitHub - trinker/textclean: Tools for cleaning and normalizing text data,](https://github.com/trinker/textclean) (3) [Quantitative Analysis of Textual Data • quanteda](https://quanteda.io/) (text analysis)

In doing this, you'll be familiarising yourself with the specific website you're interested in and figuring out where the information you want is, choosing the right [CSS-selectors](https://www.w3schools.com/cssref/css_selectors.asp), working with URLs[^4] and figuring out how to iterate (or "crawl") pages.

[^4]: See Julia Evans' "[how URLs work](https://wizardzines.com/comics/how-urls-work/)"; also see Oliver Keyes & Jay Jacobs' "[urltools](https://github.com/Ironholds/urltools)" R package --- not something I'm getting into, but I think working with URLs programmatically is the way to go!

In more complicated cases, it involves writing a script that can interact with the webpage (by driving a browser), to reveal the content you want. And if you want to build something more sophisticated, it might involve working with a database, a scheduler, and other tools.[^5] For instance, you might want to build something that extracts information regularly, tracks changes over time and stores it in a database.[^6]

[^5]: Have a look at [Heroku](https://www.heroku.com/) and their "Hobby-dev" PostgreSQL with the fee Heroku scheduler

[^6]: A bit different, but [Simon Willison's Git scraping: track changes over time by scraping to a Git repository](https://simonwillison.net/2020/Oct/9/git-scraping/) looks very interesting

------------------------------------------------------------------------

## Approach

In the examples below, I extract a table from a Wikipedia.org page (trickier than I thought!), and --- more elaborately --- extract information about jobs from a site called Gumtree (.co.za).

For the most part, the approach is something like: (a) go to the site you want to scrape, (b) figure out what the things you want to extract are called (find the right CSS-selectors using an inspect tool), (c) use the `read_html`, `html_nodes`, and other functions in the `rvest` package to extract those elements, (d) store the results in the format you want to work with (a list, a tibble, &c.), (e) figure out how to iterate (if needed), (f) troubleshoot (handling errors, dealing with broken links, missing selectors, &c.), (g) throughout: lots of Googling.

------------------------------------------------------------------------

## Considerations

There are a few things to consider before investing time in writing a webscraping script:

-   **Is this already available?** Perhaps it can be downloaded from the site as a spreadsheet already (have a look around), or perhaps the site has an API[^7] you can access to download data.
-   **Is there a low effort way of doing this?** For instance, maybe you can just copy-paste what you need into a spreadsheet or directly into R[^8]? Or maybe there's a tool you can use straight-away.[^9]
-   **How much copy-paste/ "low-effort" work are we talking here?** I think writing a simple script can, in many cases, be the lowest effort (and cheapest) of all, but one might hesitate because it feels daunting. If you'll need to copy-paste multiple things across multiple pages, rather write a script. And if it's something you plan to do regularly --- automate with a script.
-   **Is it OK for me to do this?** This is a trickier question. It depends. And sometimes it's hard to tell! Things you can do: check if the site has a `robots.txt` file[^10] (which, if a site has one, includes do's and don't for scrapers/crawlers); perhaps ask yourself "would I feel OK about this information being pulled if this was my website?". If you decide to scrape --- be "polite" about it: don't overwhelm the site. Don't bother the site more than you need to, i.e. don't send too many requests to a website per second. You can avoid this by trying to be efficient (don't repeatedly query the website); include some "rests" in your code, to slow things down.

[^7]: See [What exactly IS an API? - by Perry Eising on Medium](https://medium.com/@perrysetgo/what-exactly-is-an-api-69f36968a41f)

[^8]: See Miles McBain's "[datapasta](https://github.com/MilesMcBain/datapasta)" package

[^9]: See, for instance, [Octoparse](https://www.octoparse.com/) or [webscraper.io](https://webscraper.io/). Also see, Google Sheets' `importHTML` function (see the documentation [here](https://support.google.com/docs/answer/3093339?hl=en) or see this Martin Hawksey's tutorial [here](https://mashe.hawksey.info/2012/10/feeding-google-spreadsheets-exercises-in-import/) (looks good)

[^10]: See <https://en.wikipedia.org/wiki/Robots_exclusion_standard> then, there's helpful information in Peter Meissner's `robotstxt` R package [documentation](https://cran.r-project.org/web/packages/robotstxt/vignettes/using_robotstxt.html)

------------------------------------------------------------------------

## The front-end of the web

What you see when you open a webpage is brought to you by:

-   **HTML** -- basic content and layout semantics (adding structure to raw content)
-   **CSS** -- adding styles (formatting, colours, sizes of things, sometimes where things are placed, &c.)
-   **JavaScript** - making things interactive (behaviour of things on the page)

[**Here**](https://codepen.io/rcyou/pen/QEObEk/) is an example on [CodePen](https://codepen.io/) bringing these together.[^11]

[^11]: See <https://www.internetingishard.com/html-and-css/introduction/> and <https://www.w3schools.com/html/html_intro.asp>

You can also see how these come together by using your browser's inspect tool. On Chrome you can right click and select "inspect" or go to More Tools/ Developer Tools[^12].

[^12]: <https://developers.google.com/web/tools/chrome-devtools/open>

Here's what it looks like:

[![A screenshot of the J-PAL Africa Research webpage viewed with the Chrome inspection tool](/img/Screenshot 2020-11-12 at 10.38.08.png "A screenshot of the J-PAL Africa Research webpage viewed with the Chrome inspection tool"){#inspection-example}](https://www.povertyactionlab.org/page/research-south-africa)

------------------------------------------------------------------------

## Tools

I use a combination of the inspect tool and [SelectorGadget](http://selectorgadget.com/) ("point and click CSS selectors") to identify what the elements I want to extract are called.

I use the [rvest](https://github.com/tidyverse/rvest) R package ("rvest helps you scrape information from webpages"), and a number of [tidyverse tools](https://www.tidyverse.org/) for data manipulation. Code is heavily commented.

------------------------------------------------------------------------

## Examples

### Set up

```{r libraries}
## Load libraries ####
library(tidyverse) ##for data manipulation (various packages)
library(rvest) ##for scraping
library(textclean) ##some cleaning functions
library(robotstxt) ##to get robotstxt protocols
library(kableExtra) ##for printing out an html table in this post

```

Wikipedia has a robots.txt, and it seems OK to scrape if you're going to be polite about it:

```{r robotstxt}
## Look at robots.txt (if any) ####
rtxt_wikipedia <- robotstxt(domain="wikipedia.org")
replace_html(str_trunc(rtxt_wikipedia$text, 500, "right"))

```

As for Gumtree.co.za --- it has one, but I found it hard to interpret. I decided to just be very polite about it ¯\\\_(ツ)\_/¯

### Table example

In the first example, I extract a table from Wikipedia.

I tried using SelectorGadget and the Chrome inspection tool, but was struggling to figure out the correct selector.

Something you can do (I learned, after some Googling) is to get **all tables** on the page; then you can identify which one is the right one using string matching (e.g. the `grep` function).

[![A screenshot of the 2019 South African general election results on Wikipedia](/img/Screenshot 2020-11-14 at 20.34.27.png "A screenshot of the 2019 South African general election results on Wikipedia")](https://en.wikipedia.org/wiki/Elections_in_South_Africa)

```{r table-example}

## Get Table element from Wikipedia ####

website <- read_html("https://en.wikipedia.org/wiki/Elections_in_South_Africa") ##get the website HTML

table <- website %>% html_nodes("table") ##all tables
length(table) ##how many tables?

data <- html_table(table[grep("ANC", table)], fill = T)[[1]]

glimpse(data) ##let's see
head(data) ##great!

```

### Job posts

This is my main example: getting job information from a site called Gumtree.co.za.

[![Screenshot of job postings on gumtree.co.za (example)](/img/Screenshot 2020-11-14 at 20.47.23.png "Screenshot of job postings on gumtree.co.za")](https://www.gumtree.co.za/s-jobs/v1c8p1)

**Things to note here, after inspecting the page:** descriptions are truncated, and contact information is hidden. To reveal them, you need to click on these elements. We're not going to get into headless browsers, and how to get your script to interact with page elements, but keep in mind that this is possible.

There are multiple postings on each page, and each one links to a full ad.

I extract the links on the first page (each title links to a full post):

```{r joblinks_example1}

## Job site example ####
##reading in the HTML from this page on entry-level jobs
jobs <- read_html("https://www.gumtree.co.za/s-jobs/page-1/v1c8p2?q=entry+level")

##extract links on first page
links <-
  jobs %>%
  html_nodes(".related-ad-title") %>%
  html_attr("href")

head(links) ##great!

```

I decided to extract the job title, description, job type, employer, location, and to include a time-stamp. I used the inspect tool to identify each element's CSS-selector.

(There's much more information on this page than the few elements I chose; the approach would be the same.)

```{r jobpost-example_title}

##first link
jobs_1 <- read_html(paste0("https://www.gumtree.co.za", links[1]))

jobs_1

##get the job title
title <- jobs_1 %>%
  html_nodes("h1") %>%
  html_text() %>%
  replace_html() %>% ##strip html markup (if any)
  str_squish() ##get rid of leading and trailing whitespace

title

```

This worked.

Since I want to do this for a few more things --- title, description, &c., I code a function to make this easier, and to avoid repeating myself too much:

```{r function_clean-element}

## function for cleaning single elements ####
clean_element <- function(page_html, selector) {
  output <- page_html %>%
    html_nodes(selector) %>%
    html_text() %>%
    replace_html() %>% ##strip html markup (if any)
    str_squish() ##remove leading and trailing whitespace
  output <- ifelse(length(output) == 0, NA, output) ##in case an element is missing
  return(output)
}

```

This works (and I can always add a few more things in the function if I wanted to).

```{r jobpost-example_others}

##let's try out the function!
##I got the CSS-selectors with SelectorGadget
description <- clean_element(jobs_1, ".description-website-container") 
jobtype <- clean_element(jobs_1, ".attribute:nth-child(4) .value")
employer <- clean_element(jobs_1, ".attribute:nth-child(3) .value")
location <- clean_element(jobs_1, ".attribute:nth-child(1) .value")
time <- Sys.time()

##putting my results into a tibble (see "https://r4ds.had.co.nz/tibbles.html")
dat <-
  tibble(
    title,
    description,
    jobtype,
    location,
    time
  )

dat ##looks good!

```

What if I wanted to do this for all job posts on a page?

#### Iteration

I write another function, which will create a tibble[^13] for a job post, with the information I want. I can then use a for loop to apply the function to each job post and to put the result into a list of tibbles.

[^13]: <https://r4ds.had.co.nz/tibbles.html>

After I have a list of tibbles, I can combine them into a single dataset using `dplyr`'s `bind_rows()`.

```{r function_clean-ad}

## function for cleaning all elements I want on a page ####
clean_ad <- function(link) {
  page_html <- read_html(link) ##get HTML
  ##the list of items I want in each post
  title <- clean_element(page_html, "h1") ##parse HTML
  description <- clean_element(page_html, ".description-website-container")
  jobtype <- clean_element(page_html, ".attribute:nth-child(4) .value")
  employer <- clean_element(page_html, ".attribute:nth-child(3) .value")
  location <- clean_element(page_html, ".attribute:nth-child(1) .value")
  time <- Sys.time() ##current time

  ##I put the selected post info in a tibble
  dat <-
    tibble(
      title,
      description,
      jobtype,
      location,
      time
    )
  return(dat)
}

```

```{r iteration_example1}

## set up ####
##fixing" link addresses
links <- paste0("https://www.gumtree.co.za", links)
links <- sample(links, 10) ##sampling 10 random links for the example

##I create a list (for storing tibbles), and a for loop to iterate through links
output <- list()

## iterating through links with a for loop ####
for (i in 1:length(links)) {
  deets <- clean_ad(links[i]) ##my function for extracting selected content from a post
  output[[i]] <- deets ##add to list
  Sys.sleep(2) ##rest
}

##combining all tibbles in the list into a single big tibble
all <- bind_rows(output) ##fab!

glimpse(all)

```

```{r html-table_iteration1}

all$description <- str_trunc(all$description, 100, "right") ##truncate

##I'm using kable to make an html table for the site
all %>%
  kable() %>%
  kable_paper(bootstrap_options = c("striped", "responsive"))

```

Nice!

**How would one do other pages?**

One (hacky) approach is to use the URL and another for loop.[^14]

[^14]: There are better ways of working with URLs: <https://github.com/Ironholds/urltools>

In the example below, I extract the job links for the first 3 pages:

```{r joblinks_example2}

## set-up ####
pages <- 3 ##let's just do 3
job_list <- list()
link <- "https://www.gumtree.co.za/s-jobs/page-" ##url fragment

## iterate over pages ####
for (i in 1:pages) {
  jobs <-
    read_html(paste0(link, i, "/v1c8p2?q=entry+level")) ##using paste0
  links <-
    jobs %>%
    html_nodes(".related-ad-title") %>%
    html_attr("href") ##get links
  job_list[[i]] <- links ##add to list
  Sys.sleep(2) ##rest
}

links <- unlist(job_list) ##make a single list
links <- paste0("https://www.gumtree.co.za", links)
head(links, 10) ##looks good!

```

#### Things to think about

Make sure your script can handle errors.[^15]

[^15]: See "safely" and "possibly" from the [purrr](https://purrr.tidyverse.org/) package (or "trycatch" from base R)

-   CSS-selectors change over time (if you're planning to run your script again in future, you should check)
-   links stop working (page names change, or they stop existing)
-   not all pages are necessarily the same (some elements may be missing from some pages) --- see the "job type" variable above, for instance
-   sites time out (that can break your script if you don't account for it)
-   what other things might break your script?

#### Putting it all together

```{r all-together, eval=FALSE}

## libraries ####

library(tidyverse) ##for data manipulation (various packages)
library(rvest) ##for scraping
library(textclean) ##some cleaning functions
library(kableExtra) ##for printing out an html table in this post
library(progress) ##for tracking progress in console

## Function 1: Clean an element ####

clean_element <- function(page_html, selector) {
  output <- page_html %>%
    html_nodes(selector) %>%
    html_text() %>%
    replace_html() %>% ##strip html markup (if any)
    str_squish() ##remove leading and trailing whitespace
  output <- ifelse(length(output) == 0, NA, output) ##in case an element is missing
  return(output)
}

## Function 2: Clean an ad ####

clean_ad <- function(link) {
  page_html <- read_html(link) ##get HTML
  ##the list of items I want in each post
  title <- clean_element(page_html, "h1") ##parse HTML
  description <- clean_element(page_html, ".description-website-container")
  jobtype <- clean_element(page_html, ".attribute:nth-child(4) .value")
  employer <- clean_element(page_html, ".attribute:nth-child(3) .value")
  location <- clean_element(page_html, ".attribute:nth-child(1) .value")
  time <- Sys.time() ##current time

  ##I put the selected post info in a tibble
  dat <-
    tibble(
      title,
      description,
      jobtype,
      location,
      time
    )
  return(dat)
}

## Loop 1: Get my links ####

pages <- 5 ##let's just do 5
job_list <- list()
link <- "https://www.gumtree.co.za/s-jobs/page-" ##url fragment

for (i in 1:pages) {
  jobs <-
    read_html(paste0(link, i, "/v1c8p2?q=entry+level")) ##using paste0
  links <-
    jobs %>%
    html_nodes(".related-ad-title") %>%
    html_attr("href") ##get links
  job_list[[i]] <- links ##add to list
  Sys.sleep(2) ##rest
}

links <- unlist(job_list) ##make a single list
links <- paste0("https://www.gumtree.co.za", links)
links <- sample(links, 20) ##sampling 20

head(links)
links <- c(links, "ww.brokenlink.co.bb") ##append faulty link to test error handling

## Let's go! ####

##track progress
total <- length(links)
pb <- progress_bar$new(format = "[:bar] :current/:total (:percent)", total = total)

##error handling
safe_clean_ad <- possibly(clean_ad, NA) ##see purrr package

##list
output <- list()

##loop
for (i in 1:total) {
  pb$tick()
  deets <- safe_clean_ad(links[i])
  output[[i]] <- deets #add to list
  Sys.sleep(2) ##resting
}

##combining all tibbles in the list into a single big tibble

all <- output[!is.na(output)] ##remove empty tibbles, if any
all <- bind_rows(all) ##Fab!
glimpse(all)

```

------------------------------------------------------------------------

## Resources

-   Mine Dogucu & Mine Çetinkaya-Rundel (2020): "[Web Scraping in the Statistics and Data Science Curriculum: Challenges and Opportunities](https://github.com/mdogucu/web-scrape)", Journal of Statistics Education
-   [How we learnt to stop worrying and love web scraping \| Nature Career Column](https://www.nature.com/articles/d41586-020-02558-0) by Nicholas J. DeVito, Georgia C. Richards & Peter Inglesby
-   [Chapter 12: "Web scraping"](https://automatetheboringstuff.com/2e/chapter12/) in "Automate the Boring Stuff with Python" by Al Sweigart
-   [Git scraping: track changes over time by scraping to a Git repository](https://simonwillison.net/2020/Oct/9/git-scraping/)
-   Headless browing: [R Bindings for Selenium WebDriver • rOpenSci: RSelenium](https://docs.ropensci.org/RSelenium/)
-   Ethics/best practice
    -   [Robots exclusion standard - Wikipedia](https://en.wikipedia.org/wiki/Robots_exclusion_standard)
    -   [Introduction to robots.txt - Search Console Help](https://support.google.com/webmasters/answer/6062608?hl=en)
    -   [GitHub - dmi3kno/polite: Be nice on the web](https://github.com/dmi3kno/polite)
-   More implementation
    -   [A Crazy Little Thing Called {purrr} - Part 1 : Web Mining - Colin Fay](https://colinfay.me/purrr-web-mining/)
-   String manipulation
    -   [RegExr: Learn, Build, & Test RegEx](https://regexr.com/)
    -   [Regex Tester and Debugger Online - Javascript, PCRE, PHP](https://www.regextester.com/)
    -   R cheatsheets: [RStudio Cheatsheets - RStudio](https://rstudio.com/resources/cheatsheets/)
    -   [GitHub - kevinushey/rex: Friendly regular expressions for R.](https://github.com/kevinushey/rex)
    -   [rebus: README.md](https://rdrr.io/cran/rebus/f/README.md)
    -   [GitHub - VerbalExpressions/RVerbalExpressions: Create regular expressions easily](https://github.com/VerbalExpressions/RVerbalExpressions)
-   See also
    -   [Data science for economists \| Grant McDermott](https://www.google.com/url?q=https%3A%2F%2Fgithub.com%2Fuo-ec607%2Flectures&sa=D&sntz=1&usg=AFQjCNHFFTznBoeUT4oCXc_ILvRmBa1gnQ)
    -   [Beautiful Soup Documentation --- Beautiful Soup 4.9.0 documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
    -   [RoboBrowser: Your friendly neighborhood web scraper --- robobrowser 0.1 documentation](https://robobrowser.readthedocs.io/en/latest/readme.html)
    -   [GitHub - r-lib/httr: httr: a friendly http package for R](https://github.com/r-lib/httr)
